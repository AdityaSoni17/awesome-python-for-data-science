{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6a90a39-427f-4ec5-9ff6-75c9c9bf7140",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Time Series Imputation - Imputation with  Vanilla Seq2Seq architecture\n",
    "\n",
    "In this notebook, we explore how to impute an univariate time-series with Variational Auto Encoders. The architecture of this VAE is simple and only suited for univariate distributions. \n",
    "\n",
    "Tensorflow is the package used for the Seq2Seq model implementation.\n",
    "\n",
    "This notebooks covers the following approach:\n",
    "\n",
    "*Data Preparation:*\n",
    "\n",
    "Partition the time-series into overlapping sequences. For example, if you have a daily time-series of length 365 days and you want to use 30 days to predict the next 10 days, then you can partition the series into overlapping sequences where each sequence consists of 30 days followed by the next 10 days.\n",
    "\n",
    "In this case as we have created the missing data, the source sequence is the original data, whereas the data to be imputed is the one that has missingness created. The data was scaled, and the NaN values replaced with a mask. The chosen sequence lenght was a full week of reading 24*7, as this is a hourly dataset. \n",
    "\n",
    "*Training the Model:*\n",
    "\n",
    "Train the Seq2Seq model using the valid overlapping sequences.\n",
    "The encoder will take the 7-days source sequence and the decoder will try to reproduce the same period of time. \n",
    "\n",
    "*Challenges:*\n",
    "\n",
    "The architecture (like the number of layers, number of neurons, type of RNN cell, etc.) of the Seq2Seq model can significantly affect performance.\n",
    "The choice of the window size (7 days in the example) and how far into the future you're predicting can also be crucial.\n",
    "Overfitting can be a concern, especially if the time-series is noisy or if there's not much data available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c325f9-ce0a-4562-bfdb-b7f347b0623f",
   "metadata": {},
   "source": [
    "### Import the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f79dbe7b-369d-4f4e-9245-4468580d5af6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b388139a-11e3-4228-964d-d00cfb411f28",
   "metadata": {},
   "source": [
    "### Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "862450dd-042d-4d6a-86a5-a623bd9fedf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Utility function to generate a 3D sequence\n",
    "def gen_seq(id_df, seq_length, seq_cols):\n",
    "\n",
    "    data_matrix =  id_df[seq_cols]\n",
    "    num_elements = data_matrix.shape[0]\n",
    "\n",
    "    for start, stop in zip(range(0, num_elements-seq_length, 1), range(seq_length, num_elements, 1)):\n",
    "        \n",
    "        yield data_matrix[stop-sequence_length:stop].values.reshape((-1,len(seq_cols)))\n",
    "        \n",
    "#Scaler class\n",
    "class scaler1D:\n",
    "    \n",
    "    def fit(self, X):\n",
    "        self.mean = np.nanmean(np.asarray(X).ravel())\n",
    "        self.std = np.nanstd(np.asarray(X).ravel())\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return (X - self.mean)/self.std\n",
    "    \n",
    "    def inverse_transform(self, X):\n",
    "        return (X*self.std) + self.mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf526e5-3757-4100-a7d7-92a63959765c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc6c9957-14c5-4b94-8277-905c6b3c7bea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "og_df = pd.read_csv('spot_prices_oil.csv', index_col=[0])\n",
    "og_df[\"Date\"] = pd.to_datetime(og_df.Date, format=\"%Y-%m-%d %H:%M:%S\")\n",
    "og_df = og_df.sort_values('Date')\n",
    "\n",
    "og_df.drop_duplicates('Date', inplace=True)\n",
    "og_df.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f11d024-9906-4cda-80dc-60347b5d3be4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#read dataset with the long gaps\n",
    "long_missing = pd.read_csv('missing_long.csv', index_col=[0])\n",
    "long_missing[\"Date\"] = pd.to_datetime(long_missing.Date, format=\"%Y-%m-%d %H:%M:%S\")\n",
    "long_missing = long_missing.sort_values('Date')\n",
    "\n",
    "long_missing.drop_duplicates('Date', inplace=True)\n",
    "long_missing.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6f8b3d-7535-4af1-82f3-186be746f43a",
   "metadata": {},
   "source": [
    "## Seq2Seq Vanilla architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20209b2f-6b62-41c2-8fa5-5757b7698114",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the encoder-decoder model\n",
    "\n",
    "def create_seq2seq(input_seq_length, output_seq_length, hidden_units):\n",
    "    input_seq = Input(shape=(input_seq_length, 1))\n",
    "    encoder = LSTM(hidden_units, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder(input_seq)\n",
    "\n",
    "    decoder_input_seq = Input(shape=(output_seq_length, 1))\n",
    "    decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_input_seq, initial_state=[state_h, state_c])\n",
    "    decoder_dense = Dense(1)\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    model = Model(inputs=[input_seq, decoder_input_seq], outputs=decoder_outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6323193d-795b-4660-a554-c2f64c6b5eab",
   "metadata": {},
   "source": [
    "## Preparing the data for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ab8d275-7597-47bb-b57c-fd9e275a6c06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((34873, 168, 1), (34873, 168, 1))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Creating sequences to input into the model\n",
    "\n",
    "sequence_length = 24*7\n",
    "\n",
    "sequence_input = []\n",
    "sequence_target = []\n",
    "\n",
    "for seq in gen_seq(og_df[['BE']], sequence_length, ['BE']):\n",
    "    sequence_target.append(seq)\n",
    "    \n",
    "for seq in gen_seq(long_missing, sequence_length, ['BE']):\n",
    "    sequence_input.append(seq)\n",
    "    \n",
    "sequence_input = np.asarray(sequence_input)\n",
    "sequence_target = np.asarray(sequence_target)\n",
    "\n",
    "sequence_input.shape, sequence_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76982045-a5d1-42fc-8b3a-286a8c6c4fc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27898, 168, 1) (6975, 168, 1)\n",
      "(27898, 168, 1) (6975, 168, 1)\n"
     ]
    }
   ],
   "source": [
    "## Split the data in training and test splits\n",
    "train_size = 0.8\n",
    "\n",
    "sequence_input_train = sequence_input[:int(len(sequence_input)*train_size)]\n",
    "sequence_input_test = sequence_input[int(len(sequence_input)*train_size):]\n",
    "print(sequence_input_train.shape, sequence_input_test.shape)\n",
    "\n",
    "sequence_target_train = sequence_target[:int(len(sequence_target)*train_size)]\n",
    "sequence_target_test = sequence_target[int(len(sequence_target)*train_size):]\n",
    "print(sequence_target_train.shape, sequence_target_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a987209-f413-4421-b2a3-492a6ad4a821",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Scale the sequences and mask the missing values\n",
    "\n",
    "scaler_target = scaler1D().fit(sequence_input)\n",
    "\n",
    "sequence_input_train = scaler_target.transform(sequence_input_train)\n",
    "sequence_input_test = scaler_target.transform(sequence_input_test)\n",
    "\n",
    "sequence_target_train = scaler_target.transform(sequence_target_train)\n",
    "sequence_target_test = scaler_target.transform(sequence_target_test)\n",
    "\n",
    "mask_value = -999.\n",
    "sequence_input_train[np.isnan(sequence_input_train)] = mask_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7994d155-e4cd-46c7-b768-c6009ce0f49b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "97bae8a1-b1ae-45e1-85fe-1f1227bff854",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define input and output sequence lengths and the number of hidden units\n",
    "input_sequence_length = sequence_length  # Adjust as needed\n",
    "output_sequence_length = sequence_length  # Adjust as needed\n",
    "hidden_units = 64\n",
    "\n",
    "# Create the Seq2Seq model\n",
    "model = create_seq2seq(input_sequence_length, output_sequence_length, hidden_units)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6be2cb6-4b18-4b40-8d3f-8531b0c863c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "872/872 [==============================] - 133s 152ms/step - loss: 2.2170e-04\n",
      "Epoch 2/100\n",
      "872/872 [==============================] - 132s 152ms/step - loss: 1.3928e-04\n",
      "Epoch 3/100\n",
      "872/872 [==============================] - 132s 152ms/step - loss: 1.1321e-04\n",
      "Epoch 4/100\n",
      "872/872 [==============================] - 132s 151ms/step - loss: 9.8179e-05\n",
      "Epoch 5/100\n",
      "872/872 [==============================] - 133s 152ms/step - loss: 6.9650e-05\n",
      "Epoch 6/100\n",
      "872/872 [==============================] - 135s 155ms/step - loss: 7.0446e-05\n",
      "Epoch 7/100\n",
      "872/872 [==============================] - 131s 151ms/step - loss: 5.7391e-05\n",
      "Epoch 8/100\n",
      "872/872 [==============================] - 130s 149ms/step - loss: 5.7624e-05\n",
      "Epoch 9/100\n",
      "872/872 [==============================] - 129s 148ms/step - loss: 5.5063e-05\n",
      "Epoch 10/100\n",
      "872/872 [==============================] - 130s 149ms/step - loss: 5.1512e-05\n",
      "Epoch 11/100\n",
      "872/872 [==============================] - ETA: 0s - loss: 4.6511e-05Restoring model weights from the end of the best epoch: 1.\n",
      "872/872 [==============================] - 130s 149ms/step - loss: 4.6511e-05\n",
      "Epoch 11: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fb687e391b0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#you can add callbacks for an early stopping based on a defined metric to be monitored\n",
    "es = EarlyStopping(patience=10, verbose=1, min_delta=0.001, monitor='loss', mode='auto', restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "model.fit([sequence_input_train, sequence_target_train], sequence_target_train, epochs=100, batch_size=32, callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7a0739-09e7-447f-8298-59fce397dceb",
   "metadata": {},
   "source": [
    "## Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4508fb08-2b39-4987-8708-fcb8d04143e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218/218 [==============================] - 13s 55ms/step\n"
     ]
    }
   ],
   "source": [
    "reconstructions = model.predict([sequence_input_test, sequence_input_test])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
